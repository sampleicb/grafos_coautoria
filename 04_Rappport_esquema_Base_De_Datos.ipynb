{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "767e7e6e-07e5-4985-8b8c-c1fcd216f40f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openpyxl\n  Using cached openpyxl-3.1.5-py2.py3-none-any.whl.metadata (2.5 kB)\nCollecting et-xmlfile (from openpyxl)\n  Using cached et_xmlfile-2.0.0-py3-none-any.whl.metadata (2.7 kB)\nUsing cached openpyxl-3.1.5-py2.py3-none-any.whl (250 kB)\nUsing cached et_xmlfile-2.0.0-py3-none-any.whl (18 kB)\nInstalling collected packages: et-xmlfile, openpyxl\nSuccessfully installed et-xmlfile-2.0.0 openpyxl-3.1.5\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6a00d45-5103-4680-804b-e63036c785ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e754383a-989b-4f88-9ba6-5f62e932f409",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Base de datos **PubMED**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3bd1950-8f20-427d-b5e1-f59b2048fedc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tablas procesadas: 15\n✅ Reporte disponible en: /dbfs/FileStore/schema_report.xlsx\nDescárgalo en: https://3132215626649366.6.gcp.databricks.com/files/schema_report.xlsx\n"
     ]
    }
   ],
   "source": [
    "# 1️⃣ Librerías necesarias\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit\n",
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# 2️⃣ Define esquema/base de datos a auditar\n",
    "DATABASE = \"pubmed_db\"  # Cambia por tu esquema si aplica\n",
    "\n",
    "# 3️⃣ Lista todas las tablas del esquema\n",
    "tables = spark.catalog.listTables(DATABASE)\n",
    "\n",
    "# 4️⃣ Recorre y recolecta metadatos\n",
    "summary_data = []\n",
    "\n",
    "for t in tables:\n",
    "    table_name = t.name\n",
    "    full_table_name = f\"{DATABASE}.{table_name}\"\n",
    "    df = spark.table(full_table_name)\n",
    "    row_count = df.count()\n",
    "    schema = df.schema\n",
    "    \n",
    "    for field in schema:\n",
    "        summary_data.append({\n",
    "            \"table_name\": table_name,\n",
    "            \"column_name\": field.name,\n",
    "            \"data_type\": field.dataType.simpleString(),\n",
    "            \"row_count\": row_count\n",
    "        })\n",
    "\n",
    "print(f\"Tablas procesadas: {len(tables)}\")\n",
    "\n",
    "# 5️⃣ Pasa a Pandas para exportar fácil\n",
    "summary_pd = pd.DataFrame(summary_data)\n",
    "\n",
    "# 6️⃣ Guarda como Excel\n",
    "local_path = \"/tmp/schema_report.xlsx\"\n",
    "summary_pd.to_excel(local_path, index=False, engine=\"openpyxl\")\n",
    "\n",
    "# Copia a DBFS para descargar\n",
    "os.makedirs(\"/dbfs/FileStore\", exist_ok=True)\n",
    "dbfs_path = \"/dbfs/FileStore/schema_report.xlsx\"\n",
    "shutil.copy(local_path, dbfs_path)\n",
    "\n",
    "print(f\"✅ Reporte disponible en: {dbfs_path}\")\n",
    "print(f\"Descárgalo en: https://3132215626649366.6.gcp.databricks.com/files/schema_report.xlsx\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "57ace92e-811f-4a42-a678-70bcf72d4a7b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Decarga en CVS de base de datos **Pubmed**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de48b6ba-6768-4564-8eae-d94c512e9545",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tablas encontradas: ['author_orci', 'coautor', 'coautor_2020', 'coautor_2021', 'coautor_2022', 'coautor_2023', 'coautor_2024', 'coautor_2025', 'coautor_all', 'm_articulo', 'm_autor', 'm_revista', 'n_institucion', 'n_issn', 'n_language']\nExportando author_orci a /dbfs/FileStore/exports/json_exports/author_orci_json...\nExportando coautor a /dbfs/FileStore/exports/json_exports/coautor_json...\nExportando coautor_2020 a /dbfs/FileStore/exports/json_exports/coautor_2020_json...\nExportando coautor_2021 a /dbfs/FileStore/exports/json_exports/coautor_2021_json...\nExportando coautor_2022 a /dbfs/FileStore/exports/json_exports/coautor_2022_json...\nExportando coautor_2023 a /dbfs/FileStore/exports/json_exports/coautor_2023_json...\nExportando coautor_2024 a /dbfs/FileStore/exports/json_exports/coautor_2024_json...\nExportando coautor_2025 a /dbfs/FileStore/exports/json_exports/coautor_2025_json...\nExportando coautor_all a /dbfs/FileStore/exports/json_exports/coautor_all_json...\nExportando m_articulo a /dbfs/FileStore/exports/json_exports/m_articulo_json...\nExportando m_autor a /dbfs/FileStore/exports/json_exports/m_autor_json...\nExportando m_revista a /dbfs/FileStore/exports/json_exports/m_revista_json...\nExportando n_institucion a /dbfs/FileStore/exports/json_exports/n_institucion_json...\nExportando n_issn a /dbfs/FileStore/exports/json_exports/n_issn_json...\nExportando n_language a /dbfs/FileStore/exports/json_exports/n_language_json...\n✅ Exportación a JSON completada en DBFS.\nCopiando de DBFS (/dbfs/FileStore/exports/json_exports/) a local (/tmp/json_exports_tmp)...\nCrea ZIP en /tmp/todas_tablas_json.zip...\n✅ ZIP guardado en DBFS: /dbfs/FileStore/exports/todas_tablas_json.zip\n\uD83D\uDCC2 Descárgalo: https://<TU-WORKSPACE>.databricks.com/files/exports/todas_tablas_json.zip\n\uD83D\uDCC2 Descárgalos desde: https://3132215626649366.6.gcp.databricks.com/files/exports/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# 1️⃣ Base de datos\n",
    "DATABASE = \"pubmed_db\"\n",
    "\n",
    "# 2️⃣ Lista tablas\n",
    "tables = spark.catalog.listTables(DATABASE)\n",
    "print(f\"Tablas encontradas: {[t.name for t in tables]}\")\n",
    "\n",
    "# 3️⃣ Carpetas\n",
    "EXPORT_BASE = \"/dbfs/FileStore/exports/json_exports/\"\n",
    "LOCAL_TMP = \"/tmp/json_exports_tmp\"\n",
    "ZIP_LOCAL = \"/tmp/todas_tablas_json\"\n",
    "ZIP_DBFS = \"/dbfs/FileStore/exports/todas_tablas_json.zip\"\n",
    "\n",
    "# 4️⃣ Asegúrate de que la carpeta exista\n",
    "os.makedirs(EXPORT_BASE, exist_ok=True)\n",
    "\n",
    "# 5️⃣ Exporta cada tabla a JSON (en DBFS)\n",
    "for t in tables:\n",
    "    table_name = t.name\n",
    "    df = spark.table(f\"{DATABASE}.{table_name}\")\n",
    "    export_path = os.path.join(EXPORT_BASE, f\"{table_name}_json\")\n",
    "    print(f\"Exportando {table_name} a {export_path}...\")\n",
    "    df.coalesce(1).write.mode(\"overwrite\").json(export_path)\n",
    "\n",
    "print(\"✅ Exportación a JSON completada en DBFS.\")\n",
    "\n",
    "# 6️⃣ Limpia carpeta local\n",
    "if os.path.exists(LOCAL_TMP):\n",
    "    shutil.rmtree(LOCAL_TMP)\n",
    "os.makedirs(LOCAL_TMP, exist_ok=True)\n",
    "\n",
    "# 7️⃣ Copia todo el contenido DBFS → local\n",
    "print(f\"Copiando de DBFS ({EXPORT_BASE}) a local ({LOCAL_TMP})...\")\n",
    "shutil.copytree(\"/dbfs\" + EXPORT_BASE, LOCAL_TMP, dirs_exist_ok=True)\n",
    "\n",
    "# 8️⃣ Crear ZIP en `/tmp`\n",
    "print(f\"Crea ZIP en {ZIP_LOCAL}.zip...\")\n",
    "shutil.make_archive(ZIP_LOCAL, 'zip', LOCAL_TMP)\n",
    "\n",
    "# 9️⃣ Mueve ZIP a DBFS\n",
    "shutil.move(ZIP_LOCAL + \".zip\", ZIP_DBFS)\n",
    "\n",
    "print(f\"✅ ZIP guardado en DBFS: {ZIP_DBFS}\")\n",
    "print(f\"\uD83D\uDCC2 Descárgalo: https://<TU-WORKSPACE>.databricks.com/files/exports/todas_tablas_json.zip\")\n",
    "\n",
    "\n",
    "\n",
    "print(f\"\uD83D\uDCC2 Descárgalos desde: https://3132215626649366.6.gcp.databricks.com/files/exports/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f7c558f-c6eb-4996-a03a-2d93a56ed2c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "json_exports/ - 0.00 MB\ntodas_tablas_json.zip - 4823.70 MB\n"
     ]
    }
   ],
   "source": [
    "# Lista con tamaños en MB\n",
    "for f in dbutils.fs.ls(\"dbfs:/FileStore/exports/\"):\n",
    "    print(f\"{f.name} - {f.size / (1024*1024):.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ff0030b3-423c-4f30-a2d7-1a4b3674da0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Tabla mapeada filtrada de PubMED 2020- 2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7abc566-fe9c-437f-9f39-b44fcf46f0aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tablas procesadas: 22\n✅ Reporte disponible en: /dbfs/FileStore/schema_report_pubmeds_mapeadas.xlsx\nDescárgalo en: https://3132215626649366.6.gcp.databricks.com/files/schema_report_pubmeds_mapeadas.xlsx\n"
     ]
    }
   ],
   "source": [
    "\n",
    "DATABASE = \"default\"\n",
    "\n",
    "\n",
    "# Lista todas\n",
    "tables = spark.catalog.listTables(DATABASE)\n",
    "\n",
    "# Filtra solo la tabla deseada\n",
    "filtered_tables = [t for t in tables if t.name in ('pubmed_delta', 'pubmed_tmp_filtrado') ]\n",
    "\n",
    "\n",
    "\n",
    "# 4️⃣ Recorre y recolecta metadatos\n",
    "summary_data = []\n",
    "\n",
    "for t in tables:\n",
    "    table_name = t.name\n",
    "    full_table_name = f\"{DATABASE}.{table_name}\"\n",
    "    df = spark.table(full_table_name)\n",
    "    row_count = df.count()\n",
    "    schema = df.schema\n",
    "    \n",
    "    for field in schema:\n",
    "        summary_data.append({\n",
    "            \"table_name\": table_name,\n",
    "            \"column_name\": field.name,\n",
    "            \"data_type\": field.dataType.simpleString(),\n",
    "            \"row_count\": row_count\n",
    "        })\n",
    "\n",
    "print(f\"Tablas procesadas: {len(tables)}\")\n",
    "\n",
    "# 5️⃣ Pasa a Pandas para exportar fácil\n",
    "summary_pd = pd.DataFrame(summary_data)\n",
    "\n",
    "# 6️⃣ Guarda como Excel\n",
    "local_path = \"/tmp/schema_report.xlsx\"\n",
    "summary_pd.to_excel(local_path, index=False, engine=\"openpyxl\")\n",
    "\n",
    "# Copia a DBFS para descargar\n",
    "os.makedirs(\"/dbfs/FileStore\", exist_ok=True)\n",
    "dbfs_path = \"/dbfs/FileStore/schema_report_pubmeds_mapeadas.xlsx\"\n",
    "shutil.copy(local_path, dbfs_path)\n",
    "\n",
    "print(f\"✅ Reporte disponible en: {dbfs_path}\")\n",
    "print(f\"Descárgalo en: https://3132215626649366.6.gcp.databricks.com/files/schema_report_pubmeds_mapeadas.xlsx\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "04_Rappport_esquema_Base_De_Datos",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}